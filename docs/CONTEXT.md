# Contexte Technique - JobHub

## üéØ Objectif du Projet

JobHub r√©sout un probl√®me critique dans la recherche d'emploi : **l'arriv√©e tardive sur les offres**. Quand une offre int√©ressante est publi√©e, elle re√ßoit g√©n√©ralement 50+ candidatures en quelques heures. JobHub automatise la veille pour permettre aux utilisateurs d'√™tre parmi les premiers candidats.

### Probl√®me identifi√©
- Les offres d'alternance/stage en Data Science/IA sont tr√®s comp√©titives
- Les meilleures opportunit√©s disparaissent en quelques heures
- La veille manuelle est chronophage et inefficace
- Pas d'outil existant pour une veille automatis√©e multi-plateformes

### Solution propos√©e
Une application web qui scrape continuellement les plateformes de recrutement et notifie imm√©diatement des nouvelles offres correspondant aux crit√®res utilisateur.

## üèóÔ∏è Impl√©mentation Technique

### Frontend - Interface Utilisateur

#### Technologies choisies
- **React + Vite** : Build rapide et d√©veloppement moderne
- **JSX sans TypeScript** : Simplicit√© et rapidit√© de d√©veloppement
- **Tailwind CSS v3** : Syst√®me de design coh√©rent et responsive

#### Architecture des composants
```jsx
// Structure modulaire pour maintenabilit√©
<Layout>
  <Header />
  <main>
    <SearchForm onSubmit={handleSearch} />
    <Dashboard stats={stats} />
    <JobsList jobs={jobs} realTime={true} />
  </main>
  <Footer />
</Layout>
```

#### Design System Sp√©cifique
- **Couleurs principales** : Noir (#0A0A0B), Or (#FFD700), Blanc (#FFFFFF)
- **√âl√©ments graphiques** : Patterns g√©om√©triques, glassmorphism
- **Animations** : Micro-interactions pour feedback utilisateur
- **Responsive** : Mobile-first approach avec Tailwind

### Backend - Logique M√©tier

#### Architecture Flask
```python
# Structure modulaire pour √©volutivit√©
app/
‚îú‚îÄ‚îÄ models/          # SQLAlchemy models
‚îú‚îÄ‚îÄ routes/          # API endpoints  
‚îú‚îÄ‚îÄ services/        # Business logic
‚îú‚îÄ‚îÄ scrapers/        # Scraping modules
‚îî‚îÄ‚îÄ utils/          # Helpers & config
```

#### Syst√®me de Scraping
```python
class BaseScraper:
    def scrape(self, keywords, job_type, since_date):
        # M√©thode abstraite √† impl√©menter
        pass
    
    def parse_job(self, html_element):
        # Extraction des donn√©es communes
        return {
            'title': title,
            'company': company,
            'url': url,
            'date_posted': date,
            'location': location
        }

# Impl√©mentations sp√©cifiques
class IndeedScraper(BaseScraper): ...
class LinkedInScraper(BaseScraper): ...
class WelcomeToTheJungleScraper(BaseScraper): ...
```

#### Syst√®me de Cron Jobs
- **APScheduler** pour la gestion des t√¢ches programm√©es
- Ex√©cution toutes les 15 minutes (configurable)
- Gestion des erreurs et retry automatique
- Logs d√©taill√©s pour debugging

### Base de Donn√©es

#### Mod√®le de donn√©es
```sql
-- Table des recherches utilisateur
CREATE TABLE searches (
    id INTEGER PRIMARY KEY,
    keywords TEXT NOT NULL,
    job_types TEXT NOT NULL,  -- JSON array
    duration_minutes INTEGER DEFAULT 15,
    platforms TEXT NOT NULL, -- JSON array
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Table des offres trouv√©es
CREATE TABLE jobs (
    id INTEGER PRIMARY KEY,
    search_id INTEGER REFERENCES searches(id),
    title TEXT NOT NULL,
    company TEXT,
    url TEXT UNIQUE NOT NULL,
    platform TEXT NOT NULL,
    location TEXT,
    date_posted TIMESTAMP,
    date_found TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_new BOOLEAN DEFAULT TRUE
);

-- Table des logs d'ex√©cution
CREATE TABLE execution_logs (
    id INTEGER PRIMARY KEY,
    search_id INTEGER REFERENCES searches(id),
    platform TEXT NOT NULL,
    jobs_found INTEGER DEFAULT 0,
    execution_time FLOAT,
    status TEXT NOT NULL, -- 'success', 'error', 'partial'
    error_message TEXT,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## üîÑ Flux de Donn√©es D√©taill√©

### 1. Cr√©ation d'une Recherche
```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend
    participant DB as Database
    participant S as Scheduler

    U->>F: Remplit formulaire recherche
    F->>B: POST /api/search
    B->>DB: Sauvegarde recherche
    B->>S: Programme cron job
    S->>B: Ex√©cute scraping initial
    B->>F: Retourne r√©sultats
    F->>U: Affiche offres trouv√©es
```

### 2. Ex√©cution Automatique
```mermaid
graph TD
    A[Scheduler d√©clenche] --> B[Pour chaque plateforme]
    B --> C[Scrape nouvelles offres]
    C --> D[Compare avec base existante]
    D --> E[Filtre nouvelles offres]
    E --> F[Sauvegarde en base]
    F --> G[Log r√©sultats]
    G --> H[Notification temps r√©el]
```

## üõ°Ô∏è Gestion des D√©fis Techniques

### Anti-Bot Protection
- **Rotation des User-Agents** : Simulation de vrais navigateurs
- **D√©lais al√©atoires** : Entre les requ√™tes pour √©viter d√©tection
- **Proxies rotatifs** : Pour les gros volumes (futur)
- **Selenium avec Chrome headless** : Pour les sites JavaScript

### Rate Limiting
```python
from flask_limiter import Limiter

limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["200 per day", "50 per hour"]
)

@app.route("/api/search")
@limiter.limit("10 per minute")
def create_search(): ...
```

### Gestion des Erreurs
- **Circuit Breaker** : Arr√™t temporaire si trop d'erreurs
- **Retry Policy** : 3 tentatives avec backoff exponentiel
- **Fallback Strategies** : Scraping alternatif si plateforme down
- **Health Checks** : Monitoring continu des scrapers

### Performance
- **Caching Redis** : R√©sultats fr√©quents (futur)
- **Requ√™tes asynchrones** : aiohttp pour le scraping parall√®le
- **Pagination** : Chargement progressif des r√©sultats
- **Compression** : Gzip sur les r√©ponses API

## üîê S√©curit√© et √âthique

### Respect des Terms of Service
- **Politeness Policy** : 1-2 secondes entre requ√™tes
- **robots.txt compliance** : Respect des restrictions
- **Pas de donn√©es personnelles** : Seulement URLs publiques
- **Rate limiting strict** : √âviter surcharge des serveurs

### Protection des Donn√©es
- **Pas de stockage utilisateur** : Application stateless
- **Logs anonymis√©s** : Pas d'IP tracking
- **HTTPS obligatoire** : Chiffrement des communications
- **Input validation** : Pr√©vention injections

## üìä Monitoring et Observabilit√©

### M√©triques cl√©s
```python
# Exemple de m√©triques √† tracker
metrics = {
    'jobs_scraped_total': Counter,
    'scraping_duration_seconds': Histogram,
    'scraping_errors_total': Counter,
    'active_searches': Gauge,
    'new_jobs_found': Counter
}
```

### Logs structur√©s
```json
{
  "timestamp": "2025-08-15T10:30:00Z",
  "level": "INFO",
  "message": "Scraping completed",
  "search_id": 123,
  "platform": "indeed",
  "jobs_found": 5,
  "duration": 2.3,
  "keywords": "data science"
}
```

### Alerting
- Slack/Discord webhooks pour erreurs critiques
- Email digest quotidien des performances
- Dashboard Grafana pour m√©triques temps r√©el (futur)

## üöÄ D√©ploiement et √âvolutivit√©

### Environnements
- **Development** : SQLite + Flask dev server
- **Staging** : PostgreSQL + Gunicorn + Nginx
- **Production** : Docker + PostgreSQL + Redis + Load Balancer

### CI/CD Pipeline
```yaml
# .github/workflows/deploy.yml
on: [push, pull_request]
jobs:
  test:
    - run: pytest backend/tests/
    - run: npm test frontend/
  build:
    - run: docker build -t jobhub .
  deploy:
    if: branch == 'main'
    - run: deploy to production
```

### Monitoring Production
- **Health endpoints** : `/health`, `/ready`
- **Prometheus metrics** : Exposition des m√©triques
- **Structured logging** : ELK stack (futur)
- **Error tracking** : Sentry integration

Cette architecture technique garantit une application robuste, √©volutive et maintenant les meilleures pratiques de d√©veloppement moderne.
